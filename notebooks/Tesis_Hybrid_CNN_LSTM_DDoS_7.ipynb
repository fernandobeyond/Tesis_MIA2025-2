{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249c8d29",
   "metadata": {},
   "source": [
    "\n",
    "# Tesis — Hybrid **CNN–LSTM** para DDoS Detection\n",
    "\n",
    "Este notebook es una plantilla **DL** para tu tesis. Incluye:\n",
    "- **Construcción de secuencias** a partir de un CSV tabular (ventaneo temporal simple).\n",
    "- **Modelo híbrido CNN–LSTM** en Keras.\n",
    "- **Entrenamiento** con `EarlyStopping` y `ReduceLROnPlateau`.\n",
    "- **Curvas de aprendizaje** desde el `history`.\n",
    "- **Calibración** de probabilidades con **Temperature Scaling** (post-hoc).\n",
    "- **Confiabilidad** (reliability curve), **Brier Score**, **ECE**.\n",
    "- **Interpretabilidad** con **Integrated Gradients** (y SHAP Deep opcional si está disponible).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb982de0",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Requisitos\n",
    "\n",
    "```bash\n",
    "pip install tensorflow scikit-learn matplotlib numpy pandas\n",
    "# Opcional para interpretabilidad\n",
    "pip install shap\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2035cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Configuración inicial ===\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH  = Path(\"/mnt/data/tu_dataset.csv\")   # CSV tabular base\n",
    "TARGET_COL = \"label\"                             # 0/1\n",
    "FEATURES   = None   # Si None, se auto-infiere numéricas. O define lista: [\"f1\",\"f2\",...]\n",
    "SEQ_LEN    = 20      # largo de ventana\n",
    "SEQ_STRIDE = 5       # salto entre ventanas\n",
    "GROUP_BY   = None    # si tienes un ID de flujo/sesión, ponlo aquí. Si None, se usa orden temporal global\n",
    "\n",
    "VAL_SIZE   = 0.2\n",
    "TEST_SIZE  = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Ruta de datos:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a02af",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Carga y construcción de secuencias\n",
    "\n",
    "Estrategia simple y general:\n",
    "1. Ordenamos el dataset por tiempo si existe columna de tiempo (si no, por índice).\n",
    "2. Seleccionamos **FEATURES** numéricas.\n",
    "3. Construimos ventanas deslizantes (`SEQ_LEN`, `SEQ_STRIDE`) sobre las filas.\n",
    "4. El **label** de cada ventana será el de la última fila de la ventana (puedes ajustar a mayoría/voto si prefieres).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"No encuentro '{TARGET_COL}'. Columnas disponibles: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "# Orden temporal (ajusta si tienes 'timestamp')\n",
    "time_col = None\n",
    "for c in df.columns:\n",
    "    if \"time\" in c.lower() or \"timestamp\" in c.lower():\n",
    "        time_col = c; break\n",
    "\n",
    "if time_col:\n",
    "    df = df.sort_values(by=time_col).reset_index(drop=True)\n",
    "else:\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "# Selección de features\n",
    "if FEATURES is None:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    FEATURES = [c for c in num_cols if c != TARGET_COL]\n",
    "\n",
    "X_tab = df[FEATURES].astype(float).values\n",
    "y_all = df[TARGET_COL].astype(int).values\n",
    "\n",
    "print(\"Total filas:\", len(df), \"| Features:\", len(FEATURES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32212acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función de ventaneo\n",
    "def build_sequences(X, y, seq_len=20, stride=5):\n",
    "    xs, ys = [], []\n",
    "    for start in range(0, len(X) - seq_len + 1, stride):\n",
    "        end = start + seq_len\n",
    "        xs.append(X[start:end])\n",
    "        ys.append(y[end-1])  # etiqueta de la última fila\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_seq, y_seq = build_sequences(X_tab, y_all, SEQ_LEN, SEQ_STRIDE)\n",
    "print(\"Secuencias:\", X_seq.shape, \"| Labels:\", y_seq.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c6324",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Partición Train/Val/Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7700d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primero separamos test del final (temporalmente consistente si venimos ordenados)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_seq\n",
    ")\n",
    "\n",
    "# Luego validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=VAL_SIZE/(1-TEST_SIZE),\n",
    "    random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "class_ratio = y_train.mean()\n",
    "print(f\"Positives ratio (train): {class_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac8564",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Modelo Hybrid CNN–LSTM (Keras)\n",
    "Arquitectura típica para secuencias numéricas: `Conv1D → BatchNorm → ReLU → LSTM → Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "n_features = X_train.shape[-1]\n",
    "\n",
    "inputs = layers.Input(shape=(X_train.shape[1], n_features))\n",
    "x = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.LSTM(64, return_sequences=False)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[tf.keras.metrics.AUC(name=\"auc\"), tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                       tf.keras.metrics.Recall(name=\"recall\")])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cb496",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Entrenamiento con callbacks y class weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Class weights (útil si está desbalanceado)\n",
    "cw = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = {0: cw[0], 1: cw[1]}\n",
    "class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", factor=0.5, patience=5, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=60,\n",
    "    batch_size=256,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951edae",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Curvas de aprendizaje (history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(hist, keys=(\"loss\",\"auc\")):\n",
    "    for k in keys:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(hist.history[k])\n",
    "        plt.plot(hist.history.get(\"val_\"+k, []))\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(k)\n",
    "        plt.title(k)\n",
    "        plt.legend([\"train\", \"val\"])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(history, keys=(\"loss\",\"auc\",\"precision\",\"recall\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7414ae",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Evaluación en Test y calibración (Temperature Scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b9bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix, brier_score_loss\n",
    "import numpy as np\n",
    "\n",
    "proba_test = model.predict(X_test).ravel()\n",
    "roc_auc = roc_auc_score(y_test, proba_test)\n",
    "pr_auc  = average_precision_score(y_test, proba_test)\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f} | PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "pred_test = (proba_test >= 0.5).astype(int)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred_test, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Matriz de confusión — Test\")\n",
    "plt.colorbar()\n",
    "plt.xticks([0,1],[0,1]); plt.yticks([0,1],[0,1])\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j,i,cm[i,j],ha=\"center\",va=\"center\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "brier = brier_score_loss(y_test, proba_test)\n",
    "print(\"Brier (sin calibrar):\", brier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9dfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Temperature Scaling (optimiza T en validación para minimizar NLL)\n",
    "import tensorflow as tf\n",
    "\n",
    "val_logits = tf.math.log(model.predict(X_val).ravel() / (1 - model.predict(X_val).ravel() + 1e-12) + 1e-12)\n",
    "test_logits = tf.math.log(proba_test / (1 - proba_test + 1e-12) + 1e-12)\n",
    "\n",
    "T = tf.Variable(1.0, dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "def nll_with_temperature(T):\n",
    "    logits = val_logits / T\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_val.astype(\"float32\"), logits=logits))\n",
    "\n",
    "for _ in range(200):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll_with_temperature(T)\n",
    "    grads = tape.gradient(loss, [T])\n",
    "    optimizer.apply_gradients(zip(grads, [T]))\n",
    "\n",
    "T_opt = float(T.numpy())\n",
    "print(\"Temperatura óptima:\", T_opt)\n",
    "\n",
    "# Calibrar test\n",
    "cal_proba_test = tf.sigmoid(test_logits / T_opt).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee0d62",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Confiabilidad, Brier y ECE (antes vs después)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=15):\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    bins = np.linspace(0,1,n_bins+1); ece=0.0; total=len(y_true)\n",
    "    for i in range(n_bins):\n",
    "        l,r=bins[i],bins[i+1]\n",
    "        mask=(y_prob>=l)&(y_prob<(r if i<n_bins-1 else r))\n",
    "        if np.any(mask):\n",
    "            acc=y_true[mask].mean(); conf=y_prob[mask].mean()\n",
    "            ece += abs(acc-conf)*(mask.sum()/total)\n",
    "    return ece\n",
    "\n",
    "ece_before = expected_calibration_error(y_test, proba_test, 15)\n",
    "ece_after  = expected_calibration_error(y_test, cal_proba_test, 15)\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_before = brier_score_loss(y_test, proba_test)\n",
    "brier_after  = brier_score_loss(y_test, cal_proba_test)\n",
    "print(f\"ECE antes: {ece_before:.6f} | ECE después: {ece_after:.6f}\")\n",
    "print(f\"Brier antes: {brier_before:.6f} | Brier después: {brier_after:.6f}\")\n",
    "\n",
    "# Reliability curves\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "for name, p in {\"Original\": proba_test, \"TempScaled\": cal_proba_test}.items():\n",
    "    frac_pos, mean_pred = calibration_curve(y_test, p, n_bins=15, strategy=\"uniform\")\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\", label=name)\n",
    "plt.xlabel(\"Probabilidad promedio por bin\"); plt.ylabel(\"Fracción de positivos\")\n",
    "plt.title(\"Curva de confiabilidad — Test\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357925f",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Interpretabilidad: Integrated Gradients (básico) y SHAP Deep (opcional)\n",
    "\n",
    "> Nota: IG aquí se hace **por paso temporal** y luego se promedia a nivel de feature. Para SHAP, si está instalado, se ejecuta con `DeepExplainer` sobre un *subset* pequeño.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Integrated Gradients (sencillo)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def integrated_gradients(inputs, model, baseline=None, steps=50):\n",
    "    if baseline is None:\n",
    "        baseline = np.zeros_like(inputs)\n",
    "    interpolated = [baseline + (float(k)/steps)*(inputs - baseline) for k in range(steps+1)]\n",
    "    interpolated = np.array(interpolated)  # (steps+1, seq, feat)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        preds = model(interpolated, training=False)\n",
    "    grads = tape.gradient(preds, interpolated).numpy()\n",
    "    avg_grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    integral = (inputs - baseline) * np.mean(avg_grads, axis=0)\n",
    "    return integral  # (seq, feat)\n",
    "\n",
    "# Ejemplo sobre una muestra positiva (si existe)\n",
    "idx = np.where(y_test==1)[0]\n",
    "if len(idx)>0:\n",
    "    ex = X_test[idx[0:1]][0]  # (seq, feat)\n",
    "    ig = integrated_gradients(ex, model, baseline=None, steps=32)  # (seq, feat)\n",
    "    # Agregamos por tiempo para rankear features\n",
    "    feat_scores = ig.mean(axis=0)\n",
    "    top_idx = np.argsort(-np.abs(feat_scores))[:20]\n",
    "    print(\"Top features por |IG|:\", top_idx)\n",
    "else:\n",
    "    print(\"No hay positivos en test para ejemplo de IG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP Deep (opcional)\n",
    "try:\n",
    "    import shap\n",
    "    background = X_train[:200]\n",
    "    explainer = shap.DeepExplainer(model, background)\n",
    "    shap_values = explainer.shap_values(X_test[:100])  # cuidado con memoria\n",
    "    # Resumen: promedio sobre tiempo para obtener importancia por feature\n",
    "    import numpy as np, matplotlib.pyplot as plt\n",
    "    sv = shap_values[0] if isinstance(shap_values, list) else shap_values\n",
    "    sv_feat = np.mean(np.abs(sv), axis=1).mean(axis=0)  # (n_features,)\n",
    "    top = np.argsort(-sv_feat)[:20]\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    plt.bar(range(len(top)), sv_feat[top])\n",
    "    plt.xticks(range(len(top)), [str(i) for i in top], rotation=90)\n",
    "    plt.title(\"SHAP Deep — Importancia promedio por feature (agregada en tiempo)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"SHAP Deep no disponible o falló:\", e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
